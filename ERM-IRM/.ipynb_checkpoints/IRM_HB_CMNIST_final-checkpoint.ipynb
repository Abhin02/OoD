{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions to run this notebook\n",
    "\n",
    "In this notebook, we present the comparisons for HB-MNIST: Confounded and anti-causal colored MNIST.\n",
    "Run all the cells sequentially from top to bottom; we have commented the cells to help the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "# import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import cProfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy as cp\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_construct import * ## contains functions for constructing data \n",
    "from IRM_methods import *    ## contains IRM and ERM methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample complexity on HB-CMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr1000\n",
      "trial 0\n",
      "60000\n",
      "(232,)\n",
      "(268,)\n",
      "(225,)\n",
      "(275,)\n",
      "(5441,)\n",
      "(4559,)\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 749us/sample - loss: 1.7436 - acc: 0.6440\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s 46us/sample - loss: 1.6325 - acc: 0.7630\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s 51us/sample - loss: 1.5921 - acc: 0.7660\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s 57us/sample - loss: 1.5272 - acc: 0.7740\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s 54us/sample - loss: 1.4850 - acc: 0.7740\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s 66us/sample - loss: 1.4502 - acc: 0.7900\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s 64us/sample - loss: 1.4065 - acc: 0.8000\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 1.3620 - acc: 0.8080\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s 56us/sample - loss: 1.3267 - acc: 0.8020\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 1.2909 - acc: 0.8180\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s 50us/sample - loss: 1.2505 - acc: 0.8330\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s 62us/sample - loss: 1.2146 - acc: 0.8460\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s 56us/sample - loss: 1.1795 - acc: 0.8610\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s 59us/sample - loss: 1.1416 - acc: 0.8660\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s 56us/sample - loss: 1.1062 - acc: 0.8620\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s 55us/sample - loss: 1.0718 - acc: 0.8880\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s 57us/sample - loss: 1.0351 - acc: 0.9090\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 1.0032 - acc: 0.9090\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s 55us/sample - loss: 0.9674 - acc: 0.9280\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s 58us/sample - loss: 0.9351 - acc: 0.9290\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 0s 51us/sample - loss: 0.9032 - acc: 0.9440\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 0.8719 - acc: 0.9570\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 0.8424 - acc: 0.9570\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 0.8109 - acc: 0.9670\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.7824 - acc: 0.9720\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 0s 66us/sample - loss: 0.7558 - acc: 0.9820\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 0.7295 - acc: 0.9860\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 0.7074 - acc: 0.9900\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 0.6844 - acc: 0.9890\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 0.6622 - acc: 0.9900\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 0.6442 - acc: 0.9960\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 0.6270 - acc: 0.9950\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s 62us/sample - loss: 0.6074 - acc: 0.9960\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 0.5930 - acc: 0.9970\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 0s 59us/sample - loss: 0.5782 - acc: 0.9970\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 0.5638 - acc: 0.9970\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 0.5509 - acc: 0.9970\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 0.5395 - acc: 0.9970\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.5274 - acc: 0.9970\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 0.5175 - acc: 0.9960\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 0.5063 - acc: 0.9980\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 0.4960 - acc: 0.9990\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 0.4867 - acc: 0.9980\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.4775 - acc: 0.9980\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.4683 - acc: 0.9980\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 0.4601 - acc: 0.9970\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 0.4518 - acc: 0.9980\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.4433 - acc: 0.9980\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s 58us/sample - loss: 0.4366 - acc: 0.9980\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.4291 - acc: 0.9970\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 0.4207 - acc: 0.9970\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 0.4142 - acc: 0.9970\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 0.4083 - acc: 0.9970\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s 66us/sample - loss: 0.3993 - acc: 0.9980\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 0.3954 - acc: 0.9970\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 0.3871 - acc: 0.9980\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.3813 - acc: 0.9960\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s 64us/sample - loss: 0.3748 - acc: 0.9970\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 0.3683 - acc: 0.9980\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 0.3629 - acc: 0.9980\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 0s 58us/sample - loss: 0.3574 - acc: 0.9960\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 0.3511 - acc: 0.9980\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 0.3460 - acc: 0.9970\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 0.3406 - acc: 0.9980\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.3353 - acc: 0.9970\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 0.3298 - acc: 0.9980\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 0.3254 - acc: 0.9970\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 0s 59us/sample - loss: 0.3198 - acc: 0.9980\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.3153 - acc: 0.9980\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 0s 79us/sample - loss: 0.3109 - acc: 0.9980\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 0s 58us/sample - loss: 0.3057 - acc: 0.9980\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 0s 57us/sample - loss: 0.3015 - acc: 0.9980\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.2977 - acc: 0.9970\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s 56us/sample - loss: 0.2946 - acc: 0.9960\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.2886 - acc: 0.9980\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 0s 54us/sample - loss: 0.2855 - acc: 0.9970\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 0s 53us/sample - loss: 0.2807 - acc: 0.9980\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s 54us/sample - loss: 0.2765 - acc: 0.9980\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 0.2734 - acc: 0.9980\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 63us/sample - loss: 0.2688 - acc: 0.9980\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 0.2662 - acc: 0.9980\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 0s 47us/sample - loss: 0.2623 - acc: 0.9980\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 0.2582 - acc: 0.9980\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 0.2550 - acc: 0.9980\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 0.2514 - acc: 0.9980\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 0s 54us/sample - loss: 0.2488 - acc: 0.9970\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 0.2442 - acc: 0.9980\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 0.2421 - acc: 0.9980\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s 57us/sample - loss: 0.2389 - acc: 0.9970\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 0s 55us/sample - loss: 0.2346 - acc: 0.9980\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s 51us/sample - loss: 0.2328 - acc: 0.9980\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s 54us/sample - loss: 0.2298 - acc: 0.9980\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 0s 56us/sample - loss: 0.2264 - acc: 0.9980\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s 55us/sample - loss: 0.2232 - acc: 0.9980\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.2209 - acc: 0.9970\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 0.2172 - acc: 0.9980\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 0s 51us/sample - loss: 0.2153 - acc: 0.9970\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s 49us/sample - loss: 0.2121 - acc: 0.9970\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 0s 59us/sample - loss: 0.2099 - acc: 0.9980\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 0s 53us/sample - loss: 0.2076 - acc: 0.9970\n",
      "Training accuracy:0.9980000257492065\n",
      "Testing accuracy:0.3465999960899353\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_trial =10\n",
    "n_tr_list = [ 1000,5000, 10000, 30000, 60000] # list of training sample sizes\n",
    "\n",
    "k=0\n",
    "K = len(n_tr_list)\n",
    "ERM_model_acc = np.zeros((K,n_trial))\n",
    "ERM_model_acc_nb = np.zeros((K,n_trial))\n",
    "IRM_model_acc = np.zeros((K,n_trial))\n",
    "IRM_model_acc_v = np.zeros((K,n_trial))\n",
    "\n",
    "ERM_model_acc1 = np.zeros((K,n_trial))\n",
    "ERM_model_acc1_nb = np.zeros((K,n_trial))\n",
    "IRM_model_acc1 = np.zeros((K,n_trial))\n",
    "IRM_model_acc1_v = np.zeros((K,n_trial))\n",
    "IRM_model_ind_v = np.zeros((K,n_trial))\n",
    "\n",
    "ERM_model_acc_av = np.zeros(K)\n",
    "ERM_model_acc_av_nb = np.zeros(K)\n",
    "IRM_model_acc_av = np.zeros(K)\n",
    "IRM_model_acc_av_v = np.zeros(K)\n",
    "\n",
    "\n",
    "ERM_model_acc_av1 = np.zeros(K)\n",
    "ERM_model_acc_av1_nb = np.zeros(K)\n",
    "IRM_model_acc_av1 = np.zeros(K)\n",
    "IRM_model_acc_av1_v = np.zeros(K)\n",
    "\n",
    "list_params = []\n",
    "for n_tr in n_tr_list:\n",
    "    print (\"tr\" + str(n_tr))\n",
    "#     print (\"start\")\n",
    "    t_start = time.time()\n",
    "    for trial in range(n_trial):\n",
    "        print (\"trial \" + str(trial))\n",
    "        n_e=2\n",
    "        p_color_list = [0.2, 0.1]\n",
    "        p_label_list = [0.25]*n_e\n",
    "        D = assemble_data_mnist_confounded_child(n_tr) # initialize mnist digits data object\n",
    "\n",
    "        D.create_training_data(n_e, p_color_list, p_label_list) # creates the training environments\n",
    "\n",
    "        p_label_test = 0.25 # probability of switching pre-label in test environment\n",
    "        p_color_test = 0.9  # probability of switching the final label to obtain the color index in test environment\n",
    "\n",
    "        D.create_testing_data(p_color_test, p_label_test, n_e)  # sets up the testing environment\n",
    "        (num_examples_environment,length, width, height) = D.data_tuple_list[0][0].shape # attributes of the data\n",
    "        num_classes = len(np.unique(D.data_tuple_list[0][1])) # number of classes in the data\n",
    "\n",
    "        model_erm =  keras.Sequential([\n",
    "                keras.layers.Flatten(input_shape=(length,width,height)),\n",
    "                keras.layers.Dense(390, activation = 'relu',kernel_regularizer=keras.regularizers.l2(0.0011)),\n",
    "                keras.layers.Dense(390, activation='relu',kernel_regularizer=keras.regularizers.l2(0.0011)),\n",
    "                keras.layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        num_epochs = 100\n",
    "        batch_size = 512\n",
    "        learning_rate = 4.9e-4\n",
    "        erm_model1 = standard_erm_model(model_erm, num_epochs, batch_size, learning_rate)\n",
    "        erm_model1.fit(D.data_tuple_list)\n",
    "        erm_model1.evaluate(D.data_tuple_test)\n",
    "        print (\"Training accuracy:\" + str(erm_model1.train_acc))\n",
    "        print (\"Testing accuracy:\" + str(erm_model1.test_acc))\n",
    "        \n",
    "        ERM_model_acc[k][trial] = erm_model1.test_acc\n",
    "        ERM_model_acc1[k][trial] = erm_model1.train_acc\n",
    "\n",
    "\n",
    "#         gamma_list = [10000, 33000, 66000,100000.0]\n",
    "        gamma_list = [100000]\n",
    "        index=0\n",
    "        best_err = 1e6\n",
    "        train_list =[]\n",
    "        val_list = []\n",
    "        test_list = []\n",
    "        for gamma_new in gamma_list:\n",
    "\n",
    "            model_irm = keras.Sequential([\n",
    "                                keras.layers.Flatten(input_shape=(length,width,height)),\n",
    "                                keras.layers.Dense(390, activation = 'relu',kernel_regularizer=keras.regularizers.l2(0.0011)),\n",
    "                                keras.layers.Dense(390, activation='relu',kernel_regularizer=keras.regularizers.l2(0.0011)),\n",
    "                                keras.layers.Dense(num_classes)\n",
    "                        ])\n",
    "            batch_size       = 512\n",
    "            steps_max        = 1000\n",
    "            steps_threshold  = 190  ## threshold after which gamma_new is used\n",
    "            learning_rate    = 4.9e-4\n",
    "\n",
    "\n",
    "            irm_model1 = irm_model(model_irm, learning_rate, batch_size, steps_max, steps_threshold, gamma_new)\n",
    "            irm_model1.fit(D.data_tuple_list)\n",
    "            irm_model1.evaluate(D.data_tuple_test)\n",
    "            error_val = 1-irm_model1.val_acc\n",
    "            train_list.append(irm_model1.train_acc)\n",
    "            val_list.append(irm_model1.val_acc)\n",
    "            test_list.append(irm_model1.test_acc)\n",
    "            if(error_val<best_err):\n",
    "                index_best =index\n",
    "                best_err = error_val\n",
    "            index= index+1\n",
    "\n",
    "        print (\"Training accuracy:\" + str(train_list[index_best]))\n",
    "        print (\"Validation accuracy:\" + str(val_list[index_best]))\n",
    "        print (\"Testing accuracy:\" + str(test_list[index_best]))\n",
    "\n",
    "        IRM_model_acc_v[k][trial]  = test_list[index_best]\n",
    "        IRM_model_acc1_v[k][trial] = train_list[index_best]\n",
    "        IRM_model_ind_v[k][trial]  = index_best\n",
    "\n",
    "\n",
    "    IRM_model_acc_av_v[k] = np.mean(IRM_model_acc_v[k])\n",
    "    list_params.append([n_tr,\"IRMv_test\", np.mean(IRM_model_acc_v[k]),np.std(IRM_model_acc_v[k])])\n",
    "\n",
    "    ERM_model_acc_av[k] = np.mean(ERM_model_acc[k])\n",
    "    list_params.append([n_tr,\"ERM_test\", np.mean(ERM_model_acc[k]),np.std(ERM_model_acc[k])])\n",
    "\n",
    "\n",
    "    IRM_model_acc_av1_v[k] = np.mean(IRM_model_acc1_v[k])\n",
    "    list_params.append([n_tr,\"IRMv_train\", np.mean(IRM_model_acc1_v[k]),np.std(IRM_model_acc1_v[k])])\n",
    "    \n",
    "    ERM_model_acc_av1[k] = np.mean(ERM_model_acc1[k])\n",
    "    list_params.append([n_tr, \"ERM_train\", np.mean(ERM_model_acc1[k]),np.std(ERM_model_acc1[k])])\n",
    "\n",
    "\n",
    "    k=k+1\n",
    "\n",
    "    t_end = time.time()\n",
    "    print(\"total time: \" + str(t_end-t_start))\n",
    "results = pd.DataFrame(list_params, columns= [\"Sample\",\"Method\", \"Performance\", \"Sdev\"])\n",
    "ideal_error = np.ones(5)*0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Number of samples\", fontsize=16)\n",
    "plt.ylabel(\"Test error\", fontsize=16)\n",
    "plt.plot(n_tr_list, 1-ERM_model_acc_av, \"-r\", marker=\"+\", label=\"ERM\")\n",
    "plt.plot(n_tr_list, 1-IRM_model_acc_av_v, \"-b\", marker=\"s\",label=\"IRMv1\")\n",
    "plt.plot(n_tr_list, ideal_error, \"-g\", marker=\"x\", label=\"Optimal invariant\")\n",
    "plt.legend(loc=\"upper left\", fontsize=18)\n",
    "plt.ylim(-0.01,0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
