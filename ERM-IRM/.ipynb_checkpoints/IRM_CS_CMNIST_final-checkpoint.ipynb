{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions to run this notebook\n",
    "\n",
    "In this notebook, we present the comparisons for CS-MNIST: Covaraite shift based colored MNIST.. \n",
    "Run all the cells sequentially from top to bottom; we have commented the cells to help the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import cProfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy as cp\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_construct import * ## contains functions for constructing data \n",
    "from IRM_methods import *    ## contains IRM and ERM methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample complexity on CS-CMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr1000\n",
      "trial 0\n",
      "WARNING:tensorflow:From /Users/kartikahuja/Desktop/Python_codes/aif360v1.env/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "533/533 [==============================] - 0s 839us/sample - loss: 1.8487 - acc: 0.5291\n",
      "Epoch 2/100\n",
      "533/533 [==============================] - 0s 51us/sample - loss: 1.5220 - acc: 0.8649\n",
      "Epoch 3/100\n",
      "533/533 [==============================] - 0s 59us/sample - loss: 1.5198 - acc: 0.8649\n",
      "Epoch 4/100\n",
      "533/533 [==============================] - 0s 51us/sample - loss: 1.5119 - acc: 0.8649\n",
      "Epoch 5/100\n",
      "533/533 [==============================] - 0s 52us/sample - loss: 1.4440 - acc: 0.8649\n",
      "Epoch 6/100\n",
      "533/533 [==============================] - 0s 61us/sample - loss: 1.3561 - acc: 0.8649\n",
      "Epoch 7/100\n",
      "533/533 [==============================] - 0s 67us/sample - loss: 1.2892 - acc: 0.8649\n",
      "Epoch 8/100\n",
      "533/533 [==============================] - 0s 74us/sample - loss: 1.2442 - acc: 0.8743\n",
      "Epoch 9/100\n",
      "533/533 [==============================] - 0s 65us/sample - loss: 1.2058 - acc: 0.8874\n",
      "Epoch 10/100\n",
      "533/533 [==============================] - 0s 60us/sample - loss: 1.1641 - acc: 0.8893\n",
      "Epoch 11/100\n",
      "533/533 [==============================] - 0s 63us/sample - loss: 1.1251 - acc: 0.8874\n",
      "Epoch 12/100\n",
      "533/533 [==============================] - 0s 59us/sample - loss: 1.0967 - acc: 0.8818\n",
      "Epoch 13/100\n",
      "533/533 [==============================] - 0s 68us/sample - loss: 1.0642 - acc: 0.8856\n",
      "Epoch 14/100\n",
      "533/533 [==============================] - 0s 58us/sample - loss: 1.0218 - acc: 0.9043\n",
      "Epoch 15/100\n",
      "533/533 [==============================] - 0s 65us/sample - loss: 0.9852 - acc: 0.9250\n",
      "Epoch 16/100\n",
      "533/533 [==============================] - 0s 60us/sample - loss: 0.9620 - acc: 0.9493\n",
      "Epoch 17/100\n",
      "533/533 [==============================] - 0s 62us/sample - loss: 0.9357 - acc: 0.9531\n",
      "Epoch 18/100\n",
      "533/533 [==============================] - 0s 69us/sample - loss: 0.9036 - acc: 0.9456\n",
      "Epoch 19/100\n",
      "533/533 [==============================] - 0s 66us/sample - loss: 0.8767 - acc: 0.9493\n",
      "Epoch 20/100\n",
      "533/533 [==============================] - 0s 60us/sample - loss: 0.8519 - acc: 0.9550\n",
      "Epoch 21/100\n",
      "533/533 [==============================] - 0s 62us/sample - loss: 0.8270 - acc: 0.9568\n",
      "Epoch 22/100\n",
      "533/533 [==============================] - 0s 78us/sample - loss: 0.7976 - acc: 0.9606\n",
      "Epoch 23/100\n",
      "533/533 [==============================] - 0s 71us/sample - loss: 0.7702 - acc: 0.9681\n",
      "Epoch 24/100\n",
      "533/533 [==============================] - 0s 58us/sample - loss: 0.7503 - acc: 0.9719\n",
      "Epoch 25/100\n",
      "533/533 [==============================] - 0s 59us/sample - loss: 0.7277 - acc: 0.9719\n",
      "Epoch 26/100\n",
      "533/533 [==============================] - 0s 58us/sample - loss: 0.7122 - acc: 0.9719\n",
      "Epoch 27/100\n",
      "533/533 [==============================] - 0s 64us/sample - loss: 0.7045 - acc: 0.9756\n",
      "Epoch 28/100\n",
      "533/533 [==============================] - 0s 71us/sample - loss: 0.6890 - acc: 0.9756\n",
      "Epoch 29/100\n",
      "533/533 [==============================] - 0s 51us/sample - loss: 0.6582 - acc: 0.9794\n",
      "Epoch 30/100\n",
      "533/533 [==============================] - 0s 54us/sample - loss: 0.6396 - acc: 0.9831\n",
      "Epoch 31/100\n",
      "533/533 [==============================] - 0s 52us/sample - loss: 0.6266 - acc: 0.9850\n",
      "Epoch 32/100\n",
      "533/533 [==============================] - 0s 50us/sample - loss: 0.6109 - acc: 0.9869\n",
      "Epoch 33/100\n",
      "533/533 [==============================] - 0s 48us/sample - loss: 0.6045 - acc: 0.9850\n",
      "Epoch 34/100\n",
      "533/533 [==============================] - 0s 50us/sample - loss: 0.5947 - acc: 0.9831\n",
      "Epoch 35/100\n",
      "533/533 [==============================] - 0s 64us/sample - loss: 0.5790 - acc: 0.9850\n",
      "Epoch 36/100\n",
      "533/533 [==============================] - 0s 56us/sample - loss: 0.5685 - acc: 0.9831\n",
      "Epoch 37/100\n",
      "533/533 [==============================] - 0s 61us/sample - loss: 0.5553 - acc: 0.9906\n",
      "Epoch 38/100\n",
      "533/533 [==============================] - 0s 54us/sample - loss: 0.5598 - acc: 0.9944\n",
      "Epoch 39/100\n",
      "533/533 [==============================] - 0s 57us/sample - loss: 0.5465 - acc: 0.9944\n",
      "Epoch 40/100\n",
      "533/533 [==============================] - 0s 62us/sample - loss: 0.5308 - acc: 0.9906\n",
      "Epoch 41/100\n",
      "533/533 [==============================] - 0s 62us/sample - loss: 0.5239 - acc: 0.9869\n",
      "Epoch 42/100\n",
      "533/533 [==============================] - 0s 57us/sample - loss: 0.5105 - acc: 0.9925\n",
      "Epoch 43/100\n",
      "533/533 [==============================] - 0s 57us/sample - loss: 0.5015 - acc: 0.9944\n",
      "Epoch 44/100\n",
      "533/533 [==============================] - 0s 73us/sample - loss: 0.4940 - acc: 0.9962\n",
      "Epoch 45/100\n",
      "533/533 [==============================] - 0s 65us/sample - loss: 0.4845 - acc: 0.9962\n",
      "Epoch 46/100\n",
      "533/533 [==============================] - 0s 59us/sample - loss: 0.4792 - acc: 0.9925\n",
      "Epoch 47/100\n",
      "533/533 [==============================] - 0s 61us/sample - loss: 0.4735 - acc: 0.9944\n",
      "Epoch 48/100\n",
      "533/533 [==============================] - 0s 55us/sample - loss: 0.4665 - acc: 0.9962\n",
      "Epoch 49/100\n",
      "533/533 [==============================] - 0s 50us/sample - loss: 0.4554 - acc: 0.9981\n",
      "Epoch 50/100\n",
      "533/533 [==============================] - 0s 66us/sample - loss: 0.4488 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "533/533 [==============================] - 0s 60us/sample - loss: 0.4463 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "533/533 [==============================] - 0s 63us/sample - loss: 0.4384 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "533/533 [==============================] - 0s 57us/sample - loss: 0.4333 - acc: 0.9981\n",
      "Epoch 54/100\n",
      "533/533 [==============================] - 0s 60us/sample - loss: 0.4329 - acc: 0.9981\n",
      "Epoch 55/100\n",
      "533/533 [==============================] - 0s 67us/sample - loss: 0.4279 - acc: 0.9981\n",
      "Epoch 56/100\n",
      "533/533 [==============================] - 0s 63us/sample - loss: 0.4189 - acc: 0.9981\n",
      "Epoch 57/100\n",
      "533/533 [==============================] - 0s 54us/sample - loss: 0.4127 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "533/533 [==============================] - 0s 68us/sample - loss: 0.4093 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "533/533 [==============================] - 0s 54us/sample - loss: 0.4043 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "533/533 [==============================] - 0s 61us/sample - loss: 0.3995 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "533/533 [==============================] - 0s 50us/sample - loss: 0.3952 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "533/533 [==============================] - 0s 52us/sample - loss: 0.3913 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "533/533 [==============================] - 0s 53us/sample - loss: 0.3877 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "533/533 [==============================] - 0s 69us/sample - loss: 0.3840 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "533/533 [==============================] - 0s 52us/sample - loss: 0.3802 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "533/533 [==============================] - 0s 59us/sample - loss: 0.3762 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "533/533 [==============================] - 0s 65us/sample - loss: 0.3729 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "533/533 [==============================] - 0s 52us/sample - loss: 0.3703 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "533/533 [==============================] - 0s 55us/sample - loss: 0.3675 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "533/533 [==============================] - 0s 52us/sample - loss: 0.3627 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "533/533 [==============================] - 0s 61us/sample - loss: 0.3585 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "533/533 [==============================] - 0s 69us/sample - loss: 0.3563 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "533/533 [==============================] - 0s 54us/sample - loss: 0.3542 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "533/533 [==============================] - 0s 46us/sample - loss: 0.3505 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "533/533 [==============================] - 0s 51us/sample - loss: 0.3462 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "533/533 [==============================] - 0s 63us/sample - loss: 0.3425 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "533/533 [==============================] - 0s 56us/sample - loss: 0.3392 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "533/533 [==============================] - 0s 57us/sample - loss: 0.3360 - acc: 1.0000\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533/533 [==============================] - 0s 55us/sample - loss: 0.3330 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "533/533 [==============================] - 0s 56us/sample - loss: 0.3300 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "533/533 [==============================] - 0s 63us/sample - loss: 0.3269 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "533/533 [==============================] - 0s 58us/sample - loss: 0.3239 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "533/533 [==============================] - 0s 52us/sample - loss: 0.3208 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "533/533 [==============================] - 0s 61us/sample - loss: 0.3180 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "533/533 [==============================] - 0s 56us/sample - loss: 0.3155 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "533/533 [==============================] - 0s 52us/sample - loss: 0.3128 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "533/533 [==============================] - 0s 50us/sample - loss: 0.3097 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "533/533 [==============================] - 0s 57us/sample - loss: 0.3068 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "533/533 [==============================] - 0s 57us/sample - loss: 0.3048 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "533/533 [==============================] - 0s 62us/sample - loss: 0.3024 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "533/533 [==============================] - 0s 53us/sample - loss: 0.2993 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "533/533 [==============================] - 0s 58us/sample - loss: 0.2963 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "533/533 [==============================] - 0s 45us/sample - loss: 0.2937 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "533/533 [==============================] - 0s 63us/sample - loss: 0.2914 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "533/533 [==============================] - 0s 54us/sample - loss: 0.2891 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "533/533 [==============================] - 0s 55us/sample - loss: 0.2865 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "512/533 [===========================>..] - ETA: 0s - loss: 0.2839 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "\n",
    "n_trial =10\n",
    "n_tr_list = [1000, 5000, 10000, 30000, 60000] # list of training sample sizes\n",
    "\n",
    "k=0\n",
    "K = len(n_tr_list)\n",
    "ERM_model_acc = np.zeros((K,n_trial))\n",
    "ERM_model_acc_nb = np.zeros((K,n_trial))\n",
    "IRM_model_acc = np.zeros((K,n_trial))\n",
    "IRM_model_acc_v = np.zeros((K,n_trial))\n",
    "IRM_model_ind_v = np.zeros((K,n_trial))\n",
    "\n",
    "ERM_model_acc1 = np.zeros((K,n_trial))\n",
    "ERM_model_acc1_nb = np.zeros((K,n_trial))\n",
    "IRM_model_acc1 = np.zeros((K,n_trial))\n",
    "IRM_model_acc1_v = np.zeros((K,n_trial))\n",
    "IRM_model_ind_v = np.zeros((K,n_trial))\n",
    "\n",
    "ERM_model_acc_av = np.zeros(K)\n",
    "ERM_model_acc_av_nb = np.zeros(K)\n",
    "IRM_model_acc_av = np.zeros(K)\n",
    "IRM_model_acc_av_v = np.zeros(K)\n",
    "\n",
    "\n",
    "ERM_model_acc_av1 = np.zeros(K)\n",
    "ERM_model_acc_av1_nb = np.zeros(K)\n",
    "IRM_model_acc_av1 = np.zeros(K)\n",
    "IRM_model_acc_av1_v = np.zeros(K)\n",
    "\n",
    "list_params = []\n",
    "for n_tr in n_tr_list:\n",
    "    print (\"tr\" + str(n_tr))\n",
    "    t_start = time.time()\n",
    "    for trial in range(n_trial):\n",
    "        print (\"trial \" + str(trial))\n",
    "        n_e=2\n",
    "        p_color_list = [0.2, 0.1]\n",
    "        p_label_list = [0.25]*n_e\n",
    "        D = assemble_data_mnist_sb(n_tr) # initialize mnist digits data object\n",
    "\n",
    "        D.create_training_data(n_e, p_color_list, p_label_list) # creates the training environments\n",
    "\n",
    "        p_label_test = 0.25 # probability of switching pre-label in test environment\n",
    "        p_color_test = 0.9  # probability of switching the final label to obtain the color index in test environment\n",
    "\n",
    "        D.create_testing_data(p_color_test, p_label_test, n_e)  # sets up the testing environment\n",
    "        (num_examples_environment,length, width, height) = D.data_tuple_list[0][0].shape # attributes of the data\n",
    "        num_classes = len(np.unique(D.data_tuple_list[0][1])) # number of classes in the data\n",
    "\n",
    "        model_erm =  keras.Sequential([\n",
    "                keras.layers.Flatten(input_shape=(length,width,height)),\n",
    "                keras.layers.Dense(390, activation = 'relu',kernel_regularizer=keras.regularizers.l2(0.0011)),\n",
    "                keras.layers.Dense(390, activation='relu',kernel_regularizer=keras.regularizers.l2(0.0011)),\n",
    "                keras.layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        num_epochs = 100\n",
    "        batch_size = 512\n",
    "        learning_rate = 4.9e-4\n",
    "        erm_model1 = standard_erm_model(model_erm, num_epochs, batch_size, learning_rate)\n",
    "        erm_model1.fit(D.data_tuple_list)\n",
    "        erm_model1.evaluate(D.data_tuple_test)\n",
    "        print (\"Training accuracy:\" + str(erm_model1.train_acc))\n",
    "        print (\"Testing accuracy:\" + str(erm_model1.test_acc))\n",
    "        \n",
    "        ERM_model_acc[k][trial] = erm_model1.test_acc\n",
    "        ERM_model_acc1[k][trial] = erm_model1.train_acc\n",
    "\n",
    "\n",
    "        gamma_list = [10000, 33000, 66000,100000.0]\n",
    "        index=0\n",
    "        best_err = 1e6\n",
    "        train_list =[]\n",
    "        val_list = []\n",
    "        test_list = []\n",
    "        for gamma_new in gamma_list:\n",
    "\n",
    "            model_irm = keras.Sequential([\n",
    "                                keras.layers.Flatten(input_shape=(length,width,height)),\n",
    "                                keras.layers.Dense(390, activation = 'relu',kernel_regularizer=keras.regularizers.l2(0.0011)),\n",
    "                                keras.layers.Dense(390, activation='relu',kernel_regularizer=keras.regularizers.l2(0.0011)),\n",
    "                                keras.layers.Dense(num_classes)\n",
    "                        ])\n",
    "            batch_size       = 512\n",
    "            steps_max        = 1000\n",
    "            steps_threshold  = 190  ## threshold after which gamma_new is used\n",
    "            learning_rate    = 4.9e-4\n",
    "\n",
    "\n",
    "            irm_model1 = irm_model(model_irm, learning_rate, batch_size, steps_max, steps_threshold, gamma_new)\n",
    "            irm_model1.fit(D.data_tuple_list)\n",
    "            irm_model1.evaluate(D.data_tuple_test)\n",
    "            error_val = 1-irm_model1.val_acc\n",
    "            train_list.append(irm_model1.train_acc)\n",
    "            val_list.append(irm_model1.val_acc)\n",
    "            test_list.append(irm_model1.test_acc)\n",
    "            if(error_val<best_err):\n",
    "                index_best =index\n",
    "                best_err = error_val\n",
    "            index= index+1\n",
    "\n",
    "        print (\"Training accuracy:\" + str(train_list[index_best]))\n",
    "        print (\"Validation accuracy:\" + str(val_list[index_best]))\n",
    "        print (\"Testing accuracy:\" + str(test_list[index_best]))\n",
    "\n",
    "        IRM_model_acc_v[k][trial]  = test_list[index_best]\n",
    "        IRM_model_acc1_v[k][trial] = train_list[index_best]\n",
    "        IRM_model_ind_v[k][trial]  = index_best\n",
    "\n",
    "\n",
    "    IRM_model_acc_av_v[k] = np.mean(IRM_model_acc_v[k])\n",
    "    list_params.append([n_tr,\"IRMv_test\", np.mean(IRM_model_acc_v[k]),np.std(IRM_model_acc_v[k])])\n",
    "\n",
    "    ERM_model_acc_av[k] = np.mean(ERM_model_acc[k])\n",
    "    list_params.append([n_tr,\"ERM_test\", np.mean(ERM_model_acc[k]),np.std(ERM_model_acc[k])])\n",
    "\n",
    "\n",
    "    IRM_model_acc_av1_v[k] = np.mean(IRM_model_acc1_v[k])\n",
    "    list_params.append([n_tr,\"IRMv_train\", np.mean(IRM_model_acc1_v[k]),np.std(IRM_model_acc1_v[k])])\n",
    "    \n",
    "    ERM_model_acc_av1[k] = np.mean(ERM_model_acc1[k])\n",
    "    list_params.append([n_tr, \"ERM_train\", np.mean(ERM_model_acc1[k]),np.std(ERM_model_acc1[k])])\n",
    "\n",
    "\n",
    "    k=k+1\n",
    "\n",
    "    t_end = time.time()\n",
    "    print(\"total time: \" + str(t_end-t_start))\n",
    "    \n",
    "\n",
    "\n",
    "results = pd.DataFrame(list_params, columns= [\"Sample\",\"Method\", \"Performance\", \"Sdev\"])\n",
    "ideal_error = np.zeros(5)\n",
    "\n",
    "print (\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Number of samples\", fontsize=16)\n",
    "plt.ylabel(\"Test error\", fontsize=16)\n",
    "plt.plot(n_tr_list, 1-ERM_model_acc_av, \"-r\", marker=\"+\", label=\"ERM\")\n",
    "plt.plot(n_tr_list, 1-IRM_model_acc_av_v, \"-b\", marker=\"s\",label=\"IRMv1\")\n",
    "plt.plot(n_tr_list, ideal_error, \"-g\", marker=\"x\", label=\"Optimal invariant\")\n",
    "plt.legend(loc=\"upper left\", fontsize=18)\n",
    "plt.ylim(-0.01,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
